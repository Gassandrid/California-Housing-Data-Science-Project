---
title: "California Housing Dataset Analysis"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

We will be examining the well known **California Housing dataset**

The columns are:

- `longitude`
- `latitude`
- `housing_median_age`
- `total_rooms`
- `total_bedrooms`
- `population`
- `households`
- `median_income`
- `ocean_proximity` *(categorical string data)*
- `median_house_value`

## 1. Importing and Exploring the Data

```{r}
library(tidyverse)

url <- "https://ewan.my/datasets/housing.csv"
df <- read_csv(url)
```

---

Let's take a quick look at the first few rows and summary statistics of the dataset.

```{r}
head(df)

summary(df)

glimpse(df)
```

---

## 2. Data Cleaning and Processing

### 2.1 Checking for Missing Values

We'll check if there are any missing values in the dataset, especially in columns like `total_bedrooms`.

```{r}
missing_values <- colSums(is.na(df))
cat("Missing values in each column:\n")
print(missing_values)
```

### 2.2 Handling Missing Values

For columns with missing values (for example, `total_bedrooms`), you might decide to fill them with the median value, drop those rows, or apply another strategy.

```{r}
if (missing_values["total_bedrooms"] > 0) {
  median_bedrooms <- median(df$total_bedrooms, na.rm = TRUE)
  df$total_bedrooms <- ifelse(is.na(df$total_bedrooms), median_bedrooms, df$total_bedrooms)
  cat("Filled missing values in 'total_bedrooms' with median:", median_bedrooms, "\n")
}
```

### 2.3 Converting Categorical Data

Since `ocean_proximity` is a string, you might want to encode it before running numerical analyses or machine learning algorithms. One common method is one-hot encoding.

```{r}
df_encoded <- df %>%
  mutate(across(ocean_proximity, as.factor)) %>%
  mutate(ocean_proximity = as.factor(ocean_proximity))

ocean_dummies <- model.matrix(~ ocean_proximity - 1, data = df_encoded)
colnames(ocean_dummies) <- paste0("ocean_", levels(df_encoded$ocean_proximity))

df_encoded <- cbind(df_encoded, as.data.frame(ocean_dummies))

cat("Columns after encoding:\n")
colnames(df_encoded)
```

---

## 3. Data Visualization

### 3.1 Histogram of Housing Median Age

Let's start with a histogram to understand the distribution of the `housing_median_age`.

```{r}
ggplot(df, aes(x = housing_median_age)) +
  geom_histogram(bins = 20, color = "black", fill = "lightblue") +
  labs(
    title = "Distribution of Housing Median Age",
    x = "Housing Median Age",
    y = "Frequency"
  ) +
  theme_minimal()
```

### 3.2 Scatter Plot: Median Income vs. Median House Value

A scatter plot can be useful to observe potential relationships between `median_income` and `median_house_value`.

```{r}
ggplot(df, aes(x = median_income, y = median_house_value)) +
  geom_point(alpha = 0.5) +
  labs(
    title = "Median Income vs. Median House Value",
    x = "Median Income",
    y = "Median House Value"
  ) +
  theme_minimal()
```

### 3.3 Geographical Plot: Locations by House Value

For a more geographic perspective, you can create a scatter plot of the properties using `longitude` and `latitude`. You might color the points by `median_house_value`.

```{r}
ggplot(df, aes(x = longitude, y = latitude, color = median_house_value)) +
  geom_point(alpha = 0.6) +
  scale_color_viridis_c() +
  labs(
    title = "Geographical Distribution of House Values",
    x = "Longitude",
    y = "Latitude",
    color = "Median House Value"
  ) +
  theme_minimal()
```

---

## 4. Additional Analysis Ideas

- **Relationship Analysis:** Consider plotting total rooms vs. population, or households vs. population.
    
- **Box Plots:** To understand the distribution within groups, use box plots for variables segmented by `ocean_proximity`.
    
- **Correlation Matrix:** Generate a heatmap to visualize the correlation between different numerical variables.
    

```{r}
numeric_df <- df %>% select(where(is.numeric))
corr_matrix <- cor(numeric_df, use = "complete.obs")
cat("Correlation matrix:\n")
print(corr_matrix)

library(corrplot)
corrplot(corr_matrix, method = "circle")
```


## 5. Machine learning analysis

### 5.1 Linear Regression Model

```{r}

# Linear regression using key numerical predictors
lm_model <- lm(median_house_value ~ median_income + housing_median_age + total_rooms + total_bedrooms + population + households, data = ml_df)

# View summary of the linear regression model
summary(lm_model)

# Add predictions
ml_df$predicted_lm <- predict(lm_model)

# Plot predicted vs actual
ggplot(ml_df, aes(x = predicted_lm, y = median_house_value)) +
  geom_point(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  labs(
    title = "Linear Regression: Predicted vs Actual Median House Value",
    x = "Predicted House Value",
    y = "Actual House Value"
  ) +
  theme_minimal()
```

**Why this method?**
Linear regression is a fundamental technique used to model the relationship between a dependent variable (in this case, `median_house_value`) and one or more independent variables (such as `median_income`, `housing_median_age`, etc.). It assumes a linear relationship between the predictors and the response variable, making it interpretable and easy to implement.

**Conclusions**
The linear regression model shows that `median_income` has a strong positive relationship with `median_house_value`, indicating that as income increases, house values tend to increase as well. Interestingly, variables like total_rooms had a negative coefficient, possibly due to multicollinearity with households and population. The adjusted R-squared was about 0.5654, indicating that about 56.54% of the variation in house prices is explained by the selected predictors.



### 5.2 Classification tree
```{r}
install.packages(rpart.plot)
library(rpart)
library(rpart.plot)

# Create a binary variable for classification
ml_df <- ml_df %>%
  mutate(high_value = ifelse(median_house_value > median(median_house_value), "High", "Low"),
         high_value = as.factor(high_value))

# Fit a classification tree
class_tree <- rpart(
  high_value ~ median_income + total_rooms + population +
    `ocean_NEAR BAY` + `ocean_<1H OCEAN` + `ocean_INLAND` + `ocean_NEAR OCEAN` + `ocean_ISLAND`,
  data = ml_df,
  method = "class"
)


# Plot the tree
rpart.plot(class_tree, type = 3, extra = 104, fallen.leaves = TRUE, main = "Classification Tree for High vs Low Value Homes")

```

**Why this method?**
This classification tree was used to classify homes as either “High” or “Low” value based on whether they were above or below the median price. Classification trees are ideal for handling both categorical and numerical variables, and they produce easy-to-interpret, rule-based models. Since our data contains both categorical and numerical variables, a classification tree is a suitable choice.

**Conclusions**
The classification tree identified median_income and ocean_proximity as important variables. Homes located near the ocean or in higher-income areas were more likely to be classified as high value. This supports the idea that geographic location and socioeconomic status are key determinants of home value. The resulting tree provides an interpretable path to understand how features interact in determining class.



### 5.3 Regression Tree
```{r}
# Fit a regression tree to predict house value
reg_tree <- rpart(median_house_value ~ median_income + housing_median_age + total_rooms + total_bedrooms + population + households, 
                  data = ml_df, method = "anova")

# Plot the regression tree
rpart.plot(reg_tree, type = 3, fallen.leaves = TRUE, main = "Regression Tree for Predicting Median House Value")

```

**Why this method?**
A regression tree models a continuous output (house value) but uses a decision-tree structure to capture nonlinear relationships and interactions between variables that linear regression might miss. Regression trees are able to illustrate a relationship that a linear model may not be able to. 

**Conclusions**
This regression tree reinforces our conclusion that `median_income` is a strong predictor of house value, as it appears early in the tree. The tree structure allows us to see how different ranges of `median_income` and other features lead to different predicted house values. The model captures complex interactions between variables, such as how `total_rooms` and `population` influence house values at different income levels.


